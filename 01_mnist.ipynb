{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will do the \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understand how deep learning can solve problems traditional programming methods cannot\n",
    "* Learn about the [MNSIT handwritten digits dataset](http://yann.lecun.com/exdb/mnist/)\n",
    "* Use the [Keras API](https://keras.io/) to load the MNIST dataset and prepare it for training\n",
    "* Create a simple neural network to perform image classification\n",
    "* Train the neural network using the prepped MNIST dataset\n",
    "* Observe the performance of the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional programming, the programmer is able to articulate rules and conditions in their code that their program can then use to act in the correct way. This approach continues to work exceptionally well for a huge variety of problems.\n",
    "\n",
    "Image classification, which asks a program to correctly classify an image it has never seen before into its correct class, is near impossible to solve with traditional programming techniques. How could a programmer possibly define the rules and conditions to correctly classify a huge variety of images, especially taking into account images that they have never seen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning excels at pattern recognition by trial and error. By training a deep neural network with sufficient data, and providing the network with feedback on its performance via training, the network can identify, though a huge amount of iteration, its own set of conditions by which it can act in the correct way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the history of deep learning, the accurate image classification of the [MNSIT dataset](http://yann.lecun.com/exdb/mnist/), a collection of 70,000 grayscale images of handwritten digits from 0 to 9, was a major development. While today the problem is considered trivial, doing image classification with MNIST has become a kind of \"Hello World\" for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 40 of the images included in the MNIST dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mnist1.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Data and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with images for deep learning, we need both the images themselves, usually denoted as `X`, and also, correct [labels](https://developers.google.com/machine-learning/glossary#label) for these images, usually denoted as `Y`. Furthermore, we need `X` and `Y` values both for *training* the model, and then, a separate set of `X` and `Y` values for *validating* the performance of the model after it has been trained. Therefore, we need 4 segments of data for the MNIST dataset:\n",
    "\n",
    "1. `x_train`: Images used for training the neural network\n",
    "2. `y_train`: Correct labels for the `x_train` images, used to evaluate the model's predictions during training\n",
    "3. `x_valid`: Images set aside for validating the performance of the model after it has been trained\n",
    "4. `y_valid`: Correct labels for the `x_valid` images, used to evaluate the model's predictions after it has been trained\n",
    "\n",
    "The process of preparing data for analysis is called [Data Engineering](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7). To learn more about the differences between training data and validation data (as well as test data), check out [this article](https://machinelearningmastery.com/difference-test-validation-datasets/) by Jason Brownlee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data Into Memory (with Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [deep learning frameworks](https://developer.nvidia.com/deep-learning-frameworks), each with their own merits. In this workshop we will be working with [Tensorflow 2](https://www.tensorflow.org/tutorials/quickstart/beginner), and specifically with the [Keras API](https://keras.io/). Keras has many useful built in functions designed for the computer vision tasks. It is also a legitimate choice for deep learning in a professional setting due to its [readability](https://blog.pragmaticengineer.com/readable-code/) and efficiency, though it is not alone in this regard, and it is worth investigating a variety of frameworks when beginning a deep learning project.\n",
    "\n",
    "One of the many helpful features that Keras provides are modules containing many helper methods for [many common datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), including MNIST.\n",
    "\n",
    "We will begin by loading the Keras dataset module for MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `mnist` module, we can easily load the MNIST data, already partitioned into images and labels for both training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and validation sets\n",
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated above that the MNIST dataset contained 70,000 grayscale images of handwritten digits. By executing the following cells, we can see that Keras has partitioned 60,000 of these images for training, and 10,000 for validation (after training), and also, that each image itself is a 2D array with the dimensions 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that these 28x28 images are represented as a collection of unsigned 8-bit integer values between 0 and 255, the values corresponding with a pixel's grayscale value where `0` is black, `255` is white, and all other values are in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Matplotlib](https://matplotlib.org/), we can render one of these grayscale images in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f1fd61828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = x_train[0]\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we can now see that this is a 28x28 pixel image of a 5. Or is it a 3? The answer is in the `y_train` data, which contains correct labels for the data. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, it is common that data needs to be transformed to be in the ideal state for training. For this particular image classification problem, there are 3 tasks we should perform with the data in preparation for training:\n",
    "1. Flatten the image data, to simplify the image input into the model\n",
    "2. Normalize the image data, to make the image input values easier to work with for the model\n",
    "3. Categorize the labels, to make the label values easier to work with for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's possible for a deep learning model to accept a 2-dimensional image (in our case 28x28 pixels), we're going to simplify things to start and [reshape](https://www.tensorflow.org/api_docs/python/tf/reshape) each image into a single array of 784 continuous pixels (note: 28x28 = 784). This is also called flattening the image.\n",
    "\n",
    "Here we accomplish this using the helper method `reshape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_valid = x_valid.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the image data has been reshaped and is now a collection of 1D arrays containing 784 pixel values each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models are better at dealing with floating point numbers between 0 and 1 (more on this topic later). Converting integer values to floating point values between 0 and 1 is called [normalization](https://developers.google.com/machine-learning/glossary#normalization), and a simple approach we will take here to normalize the data will be to divide all the pixel values (which if you recall are between 0 and 255) by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the values are all floating point values between `0.0` and `1.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider for a moment, if we were to ask, what is 7 - 2? Stating that the answer was 4 is closer than stating that the answer was 9. However, for this image classification problem, we don't want the neural network to learn this kind of reasoning: we just want it to select the correct category, and understand that if we have an image of the number 5, that guessing 4 is just as bad as guessing 9.\n",
    "\n",
    "As it stands, the labels for the images are integers between 0 and 9. Because these values represent a numerical range, the model might try to draw some conclusions about its performance based on how close to the correct numerical category it guesses.\n",
    "\n",
    "Therefore, we will do something to our data called categorical encoding. This kind of transformation modifies the data so that each value is a collection of all possible categories, with the actual category that this particular value is set as true.\n",
    "\n",
    "As a simple example, consider if we had 3 categories: red, blue, and green. For a given color, 2 of these categories would be false, and the other would be true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Actual Color| Is Red? | Is Blue? | Is Green?|\n",
    "|------------|---------|----------|----------|\n",
    "|Red|True|False|False|\n",
    "|Green|False|False|True|\n",
    "|Blue|False|True|False|\n",
    "|Green|False|False|True|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use \"True\" or \"False\", we could represent the same using binary, either 0 or 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Actual Color| Is Red? | Is Blue? | Is Green?|\n",
    "|------------|---------|----------|----------|\n",
    "|Red|1|0|0|\n",
    "|Green|0|0|1|\n",
    "|Blue|0|1|0|\n",
    "|Green|0|0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what categorical encoding is, transforming values which are intended to be understood as categorical labels into a representation that makes their categorical nature explicit to the model. Thus, if we were using these values for training, we would convert..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "values = ['red, green, blue, green']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which a neural network would have a very difficult time making sense of, instead to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "values = [\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorically Encoding the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a utility to [categorically encode values](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical), and here we use it to perform categorical encoding for both the training and validation labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "num_categories = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_categories)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 values of the training labels, which you can see have now been categorically encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared for training, it is now time to create the model that we will train with the data. This first basic model will be made up of several *layers* and will be comprised of 3 main parts:\n",
    "\n",
    "1. An input layer, which will receive data in some expected format\n",
    "2. Several [hidden layers](https://developers.google.com/machine-learning/glossary#hidden-layer), each comprised of many *neurons*. Each [neuron](https://developers.google.com/machine-learning/glossary#neuron) will have the ability to affect the network's guess with its *weights*, which are values that will be updated over many iterations as the network gets feedback on its performance and learns\n",
    "3. An output layer, which will depict the network's guess for a given image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will use Keras's [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model class to instantiate an instance of a model that will have a series of layers that data will pass through in sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the input layer. This layer will be *densely connected*, meaning that each neuron in it, and its weights, will affect every neuron in the next layer. To do this with Keras, we use Keras's [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `units` argument specifies the number of neurons in the layer. We are going to use `512` which we have chosen from experimentation. Choosing the correct number of neurons is what puts the \"science\" in \"data science\" as it is a matter of capturing the statistical complexity of the dataset. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.\n",
    "\n",
    "We will learn more about activation functions later, but for now, we will use the `relu` activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function.\n",
    "\n",
    "The `input_shape` value specifies the shape of the incoming data which in our situation is a 1D array of 784 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=512, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add an additional densely connected layer. Again, much more will be said about these later, but for now know that these layers give the network more parameters to contribute towards its guesses, and therefore, more subtle opportunities for accurate learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 512, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add an output layer. This layer uses the activation function `softmax` which will result in each of the layer's values being a probability between 0 and 1 and will result in all the outputs of the layer adding to 1. In this case, since the network is to make a guess about a single image belonging to 1 of 10 possible categories, there will be 10 outputs. Each output gives the model's guess (a probability) that the image belongs to that specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the model instance method [summary](https://www.tensorflow.org/api_docs/python/tf/summary) which will print a readable summary of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of trainable parameters. Each of these can be adjusted during training and will contribute towards the trained model's guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, more details are to follow, but the final step we need to do before we can actually train our model with data is to [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) it. Here we specify a [loss function](https://developers.google.com/machine-learning/glossary#loss) which will be used for the model to understand how well it is performing during training. We also specify that we would like to track `accuracy` while the model trains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared training and validation data, and a model, it's time to train our model with our training data, and verify it with its validation data.\n",
    "\n",
    "\"Training a model with data\" is often also called \"fitting a model to data.\" Put this latter way, it highlights that the shape of the model changes over time to more accurately understand the data that it is being given.\n",
    "\n",
    "When fitting (training) a model with Keras, we use the model's [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method. It expects the following arguments:\n",
    "\n",
    "* The training data\n",
    "* The labels for the training data\n",
    "* The number of times it should train on the entire training dataset (called an *epoch*)\n",
    "* The validation or test data, and its labels\n",
    "\n",
    "Run the cell below to train the model. We will discuss its output after the training completes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1916 - accuracy: 0.9438 - val_loss: 0.1028 - val_accuracy: 0.9714\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1014 - accuracy: 0.9740 - val_loss: 0.1247 - val_accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0842 - accuracy: 0.9798 - val_loss: 0.1202 - val_accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0716 - accuracy: 0.9833 - val_loss: 0.1118 - val_accuracy: 0.9795\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0686 - accuracy: 0.9856 - val_loss: 0.1366 - val_accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, verbose=1, validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 5 epochs, notice the `accuracy` and `val_accuracy` scores. `accuracy` states how well the model did for the epoch on all the training data. `val_accuracy` states how well the model did on the validation data, which if you recall, was not used at all for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did quite well! The accuracy quickly reached close to 100%, as did the validation accuracy. We now have a model that can be used to accurately detect and classify hand-written images.\n",
    "\n",
    "The next step would be to use this model to classify new not-yet-seen handwritten images. This is called [inference](https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/). We'll explore the process of inference in a later exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a moment to appreciate what we've done here. Historically, the expert systems that were built to do this kind of task were extremely complicated, and people spent their careers building them (check out the references on the [official MNIST page](http://yann.lecun.com/exdb/mnist/) and the years milestones were reached).\n",
    "\n",
    "MNIST is not only useful for its historical influence on Computer Vision, but it's also a great [benchmark](http://www.cs.toronto.edu/~serailhydra/publications/tbd-iiswc18.pdf) and debugging tool. Having trouble getting a fancy new machine learning architecture working? Check it against MNIST. If it can't learn on this dataset, chances are it won't learn on more complicated images and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the GPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you learned how to build and train a simple neural network for image classification. In the next section, you will be asked to build your own neural network and perform data preparation to solve a different image classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ☆ Bonus Exercise ☆\n",
    "\n",
    "Have time to spare? In the next section, we will talk about how we arrived at some of the numbers above, but we can try imagining what it was like to be a researcher developing the techniques commonly used today.\n",
    "\n",
    "Ultimately, each neuron is trying to fit a line to some data. Below, we have some datapoints and a randomly drawn line using the equation [y = mx + b](https://www.mathsisfun.com/equation_of_line.html).\n",
    "\n",
    "Try changing the `m` and the `b` in order to find the lowest possible loss. How did you find the best line? Can you make a program to follow your strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.42813674185417\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 5.3408756  # -2 to start, change me please\n",
    "b = 13.077126  # 40 to start, change me please\n",
    "\n",
    "# 5.6 12.1 16.8\n",
    "\n",
    "# Sample data\n",
    "x = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9])\n",
    "y = np.array([10, 20, 25, 30, 40, 45, 40, 50, 60, 55])\n",
    "y_hat = x * m + b\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, y_hat, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss:\", np.sum((y - y_hat)**2)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 909us/step - loss: 1718.1439 - accuracy: 0.0000e+00\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 730us/step - loss: 350.7449 - accuracy: 0.0000e+00\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 662us/step - loss: 114.0409 - accuracy: 0.0000e+00\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 655us/step - loss: 72.6839 - accuracy: 0.0000e+00\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 65.0809 - accuracy: 0.0000e+00\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 674us/step - loss: 63.3145 - accuracy: 0.0000e+00\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 670us/step - loss: 62.5615 - accuracy: 0.0000e+00\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 664us/step - loss: 61.9886 - accuracy: 0.0000e+00\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 689us/step - loss: 61.4518 - accuracy: 0.0000e+00\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 675us/step - loss: 60.9262 - accuracy: 0.0000e+00\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 743us/step - loss: 60.4074 - accuracy: 0.0000e+00\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 792us/step - loss: 59.8946 - accuracy: 0.0000e+00\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 655us/step - loss: 59.3876 - accuracy: 0.0000e+00\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 748us/step - loss: 58.8863 - accuracy: 0.0000e+00\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 742us/step - loss: 58.3906 - accuracy: 0.0000e+00\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 765us/step - loss: 57.9006 - accuracy: 0.0000e+00\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 647us/step - loss: 57.4161 - accuracy: 0.0000e+00\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 646us/step - loss: 56.9370 - accuracy: 0.0000e+00\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 648us/step - loss: 56.4633 - accuracy: 0.0000e+00\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 646us/step - loss: 55.9949 - accuracy: 0.0000e+00\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 755us/step - loss: 55.5318 - accuracy: 0.0000e+00\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 722us/step - loss: 55.0740 - accuracy: 0.0000e+00\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 654us/step - loss: 54.6213 - accuracy: 0.0000e+00\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 658us/step - loss: 54.1737 - accuracy: 0.0000e+00\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 639us/step - loss: 53.7311 - accuracy: 0.0000e+00\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 754us/step - loss: 53.2935 - accuracy: 0.0000e+00\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 654us/step - loss: 52.8609 - accuracy: 0.0000e+00\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 676us/step - loss: 52.4331 - accuracy: 0.0000e+00\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 639us/step - loss: 52.0101 - accuracy: 0.0000e+00\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 645us/step - loss: 51.5920 - accuracy: 0.0000e+00\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 656us/step - loss: 51.1784 - accuracy: 0.0000e+00\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 672us/step - loss: 50.7696 - accuracy: 0.0000e+00\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 651us/step - loss: 50.3654 - accuracy: 0.0000e+00\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 686us/step - loss: 49.9657 - accuracy: 0.0000e+00\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 769us/step - loss: 49.5705 - accuracy: 0.0000e+00\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 689us/step - loss: 49.1798 - accuracy: 0.0000e+00\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 685us/step - loss: 48.7935 - accuracy: 0.0000e+00\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 663us/step - loss: 48.4115 - accuracy: 0.0000e+00\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 649us/step - loss: 48.0339 - accuracy: 0.0000e+00\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 662us/step - loss: 47.6605 - accuracy: 0.0000e+00\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 649us/step - loss: 47.2912 - accuracy: 0.0000e+00\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 652us/step - loss: 46.9262 - accuracy: 0.0000e+00\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 667us/step - loss: 46.5653 - accuracy: 0.0000e+00\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 668us/step - loss: 46.2084 - accuracy: 0.0000e+00\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 694us/step - loss: 45.8555 - accuracy: 0.0000e+00\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 684us/step - loss: 45.5066 - accuracy: 0.0000e+00\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 779us/step - loss: 45.1617 - accuracy: 0.0000e+00\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 696us/step - loss: 44.8206 - accuracy: 0.0000e+00\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 659us/step - loss: 44.4834 - accuracy: 0.0000e+00\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 708us/step - loss: 44.1500 - accuracy: 0.0000e+00\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 707us/step - loss: 43.8203 - accuracy: 0.0000e+00\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 43.4944 - accuracy: 0.0000e+00\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 796us/step - loss: 43.1721 - accuracy: 0.0000e+00\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 727us/step - loss: 42.8534 - accuracy: 0.0000e+00\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 796us/step - loss: 42.5383 - accuracy: 0.0000e+00\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 755us/step - loss: 42.2268 - accuracy: 0.0000e+00\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 818us/step - loss: 41.9188 - accuracy: 0.0000e+00\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 703us/step - loss: 41.6143 - accuracy: 0.0000e+00\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 661us/step - loss: 41.3132 - accuracy: 0.0000e+00\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 692us/step - loss: 41.0154 - accuracy: 0.0000e+00\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 750us/step - loss: 40.7211 - accuracy: 0.0000e+00\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 912us/step - loss: 40.4300 - accuracy: 0.0000e+00\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 818us/step - loss: 40.1422 - accuracy: 0.0000e+00\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 797us/step - loss: 39.8577 - accuracy: 0.0000e+00\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 677us/step - loss: 39.5764 - accuracy: 0.0000e+00\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 700us/step - loss: 39.2982 - accuracy: 0.0000e+00\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 896us/step - loss: 39.0232 - accuracy: 0.0000e+00\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 698us/step - loss: 38.7512 - accuracy: 0.0000e+00\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 882us/step - loss: 38.4824 - accuracy: 0.0000e+00\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 720us/step - loss: 38.2165 - accuracy: 0.0000e+00\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 746us/step - loss: 37.9537 - accuracy: 0.0000e+00\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 754us/step - loss: 37.6938 - accuracy: 0.0000e+00\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 674us/step - loss: 37.4369 - accuracy: 0.0000e+00\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 955us/step - loss: 37.1828 - accuracy: 0.0000e+00\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 805us/step - loss: 36.9316 - accuracy: 0.0000e+00\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 678us/step - loss: 36.6832 - accuracy: 0.0000e+00\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 949us/step - loss: 36.4377 - accuracy: 0.0000e+00\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 788us/step - loss: 36.1948 - accuracy: 0.0000e+00\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 829us/step - loss: 35.9548 - accuracy: 0.0000e+00\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 665us/step - loss: 35.7174 - accuracy: 0.0000e+00\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 673us/step - loss: 35.4827 - accuracy: 0.0000e+00\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 697us/step - loss: 35.2506 - accuracy: 0.0000e+00\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 757us/step - loss: 35.0212 - accuracy: 0.0000e+00\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 836us/step - loss: 34.7943 - accuracy: 0.0000e+00\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 671us/step - loss: 34.5700 - accuracy: 0.0000e+00\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 716us/step - loss: 34.3483 - accuracy: 0.0000e+00\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 980us/step - loss: 34.1290 - accuracy: 0.0000e+00\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 707us/step - loss: 33.9122 - accuracy: 0.0000e+00\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 699us/step - loss: 33.6978 - accuracy: 0.0000e+00\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 774us/step - loss: 33.4859 - accuracy: 0.0000e+00\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 829us/step - loss: 33.2763 - accuracy: 0.0000e+00\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 812us/step - loss: 33.0691 - accuracy: 0.0000e+00\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 773us/step - loss: 32.8642 - accuracy: 0.0000e+00\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 735us/step - loss: 32.6616 - accuracy: 0.0000e+00\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 712us/step - loss: 32.4614 - accuracy: 0.0000e+00\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 794us/step - loss: 32.2633 - accuracy: 0.0000e+00\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 715us/step - loss: 32.0675 - accuracy: 0.0000e+00\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 900us/step - loss: 31.8739 - accuracy: 0.0000e+00\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 813us/step - loss: 31.6825 - accuracy: 0.0000e+00\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 687us/step - loss: 31.4933 - accuracy: 0.0000e+00\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 891us/step - loss: 31.3062 - accuracy: 0.0000e+00\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 760us/step - loss: 31.1211 - accuracy: 0.0000e+00\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 768us/step - loss: 30.9382 - accuracy: 0.0000e+00\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 914us/step - loss: 30.7573 - accuracy: 0.0000e+00\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 805us/step - loss: 30.5785 - accuracy: 0.0000e+00\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 831us/step - loss: 30.4017 - accuracy: 0.0000e+00\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 841us/step - loss: 30.2269 - accuracy: 0.0000e+00\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 753us/step - loss: 30.0540 - accuracy: 0.0000e+00\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 804us/step - loss: 29.8831 - accuracy: 0.0000e+00\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.7141 - accuracy: 0.0000e+00\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 956us/step - loss: 29.5470 - accuracy: 0.0000e+00\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 791us/step - loss: 29.3818 - accuracy: 0.0000e+00\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 919us/step - loss: 29.2185 - accuracy: 0.0000e+00\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 742us/step - loss: 29.0570 - accuracy: 0.0000e+00\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 660us/step - loss: 28.8973 - accuracy: 0.0000e+00\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 738us/step - loss: 28.7394 - accuracy: 0.0000e+00\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 712us/step - loss: 28.5833 - accuracy: 0.0000e+00\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 926us/step - loss: 28.4289 - accuracy: 0.0000e+00\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 685us/step - loss: 28.2763 - accuracy: 0.0000e+00\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 740us/step - loss: 28.1254 - accuracy: 0.0000e+00\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.9762 - accuracy: 0.0000e+00\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 710us/step - loss: 27.8287 - accuracy: 0.0000e+00\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 27.6829 - accuracy: 0.0000e+00\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 709us/step - loss: 27.5387 - accuracy: 0.0000e+00\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 819us/step - loss: 27.3961 - accuracy: 0.0000e+00\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 713us/step - loss: 27.2551 - accuracy: 0.0000e+00\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 695us/step - loss: 27.1157 - accuracy: 0.0000e+00\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 934us/step - loss: 26.9779 - accuracy: 0.0000e+00\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 943us/step - loss: 26.8416 - accuracy: 0.0000e+00\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 986us/step - loss: 26.7069 - accuracy: 0.0000e+00\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 690us/step - loss: 26.5737 - accuracy: 0.0000e+00\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 725us/step - loss: 26.4420 - accuracy: 0.0000e+00\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.3117 - accuracy: 0.0000e+00\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 682us/step - loss: 26.1830 - accuracy: 0.0000e+00\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 739us/step - loss: 26.0556 - accuracy: 0.0000e+00\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 670us/step - loss: 25.9298 - accuracy: 0.0000e+00\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 854us/step - loss: 25.8053 - accuracy: 0.0000e+00\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 728us/step - loss: 25.6822 - accuracy: 0.0000e+00\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.5606 - accuracy: 0.0000e+00\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 651us/step - loss: 25.4403 - accuracy: 0.0000e+00\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.3213 - accuracy: 0.0000e+00\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.2037 - accuracy: 0.0000e+00\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 848us/step - loss: 25.0874 - accuracy: 0.0000e+00\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 745us/step - loss: 24.9724 - accuracy: 0.0000e+00\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 711us/step - loss: 24.8588 - accuracy: 0.0000e+00\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 942us/step - loss: 24.7464 - accuracy: 0.0000e+00\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 890us/step - loss: 24.6352 - accuracy: 0.0000e+00\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 678us/step - loss: 24.5253 - accuracy: 0.0000e+00\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 921us/step - loss: 24.4167 - accuracy: 0.0000e+00\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 952us/step - loss: 24.3093 - accuracy: 0.0000e+00\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 709us/step - loss: 24.2031 - accuracy: 0.0000e+00\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 704us/step - loss: 24.0980 - accuracy: 0.0000e+00\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 702us/step - loss: 23.9942 - accuracy: 0.0000e+00\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 898us/step - loss: 23.8915 - accuracy: 0.0000e+00\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 23.7900 - accuracy: 0.0000e+00\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 787us/step - loss: 23.6897 - accuracy: 0.0000e+00\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 736us/step - loss: 23.5904 - accuracy: 0.0000e+00\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 974us/step - loss: 23.4923 - accuracy: 0.0000e+00\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 751us/step - loss: 23.3953 - accuracy: 0.0000e+00\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 665us/step - loss: 23.2994 - accuracy: 0.0000e+00\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 698us/step - loss: 23.2045 - accuracy: 0.0000e+00\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 696us/step - loss: 23.1108 - accuracy: 0.0000e+00\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 749us/step - loss: 23.0181 - accuracy: 0.0000e+00\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 886us/step - loss: 22.9264 - accuracy: 0.0000e+00\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 823us/step - loss: 22.8358 - accuracy: 0.0000e+00\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 670us/step - loss: 22.7461 - accuracy: 0.0000e+00\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 952us/step - loss: 22.6575 - accuracy: 0.0000e+00\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 734us/step - loss: 22.5699 - accuracy: 0.0000e+00\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 951us/step - loss: 22.4833 - accuracy: 0.0000e+00\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 752us/step - loss: 22.3977 - accuracy: 0.0000e+00\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 964us/step - loss: 22.3130 - accuracy: 0.0000e+00\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 869us/step - loss: 22.2292 - accuracy: 0.0000e+00\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 716us/step - loss: 22.1465 - accuracy: 0.0000e+00\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 841us/step - loss: 22.0646 - accuracy: 0.0000e+00\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 946us/step - loss: 21.9837 - accuracy: 0.0000e+00\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 699us/step - loss: 21.9036 - accuracy: 0.0000e+00\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 673us/step - loss: 21.8245 - accuracy: 0.0000e+00\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 919us/step - loss: 21.7463 - accuracy: 0.0000e+00\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 693us/step - loss: 21.6689 - accuracy: 0.0000e+00\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 813us/step - loss: 21.5925 - accuracy: 0.0000e+00\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 826us/step - loss: 21.5169 - accuracy: 0.0000e+00\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 707us/step - loss: 21.4421 - accuracy: 0.0000e+00\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.3682 - accuracy: 0.0000e+00\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 958us/step - loss: 21.2951 - accuracy: 0.0000e+00\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 696us/step - loss: 21.2228 - accuracy: 0.0000e+00\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 676us/step - loss: 21.1514 - accuracy: 0.0000e+00\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 947us/step - loss: 21.0807 - accuracy: 0.0000e+00\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 691us/step - loss: 21.0109 - accuracy: 0.0000e+00\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 708us/step - loss: 20.9418 - accuracy: 0.0000e+00\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 840us/step - loss: 20.8735 - accuracy: 0.0000e+00\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 765us/step - loss: 20.8060 - accuracy: 0.0000e+00\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 688us/step - loss: 20.7393 - accuracy: 0.0000e+00\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 686us/step - loss: 20.6733 - accuracy: 0.0000e+00\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 712us/step - loss: 20.6080 - accuracy: 0.0000e+00\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 910us/step - loss: 20.5435 - accuracy: 0.0000e+00\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 756us/step - loss: 20.4797 - accuracy: 0.0000e+00\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 830us/step - loss: 20.4166 - accuracy: 0.0000e+00\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 830us/step - loss: 20.3542 - accuracy: 0.0000e+00\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 687us/step - loss: 20.2926 - accuracy: 0.0000e+00\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 694us/step - loss: 20.2316 - accuracy: 0.0000e+00\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 823us/step - loss: 20.1713 - accuracy: 0.0000e+00\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 852us/step - loss: 20.1117 - accuracy: 0.0000e+00\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 745us/step - loss: 20.0527 - accuracy: 0.0000e+00\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 828us/step - loss: 19.9945 - accuracy: 0.0000e+00\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 765us/step - loss: 19.9369 - accuracy: 0.0000e+00\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.8799 - accuracy: 0.0000e+00\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.8236 - accuracy: 0.0000e+00\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 19.7679 - accuracy: 0.0000e+00\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 752us/step - loss: 19.7128 - accuracy: 0.0000e+00\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 675us/step - loss: 19.6584 - accuracy: 0.0000e+00\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 811us/step - loss: 19.6045 - accuracy: 0.0000e+00\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 829us/step - loss: 19.5513 - accuracy: 0.0000e+00\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 762us/step - loss: 19.4987 - accuracy: 0.0000e+00\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 858us/step - loss: 19.4467 - accuracy: 0.0000e+00\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 806us/step - loss: 19.3952 - accuracy: 0.0000e+00\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 764us/step - loss: 19.3443 - accuracy: 0.0000e+00\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 781us/step - loss: 19.2941 - accuracy: 0.0000e+00\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 668us/step - loss: 19.2443 - accuracy: 0.0000e+00\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 785us/step - loss: 19.1952 - accuracy: 0.0000e+00\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 679us/step - loss: 19.1465 - accuracy: 0.0000e+00\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 932us/step - loss: 19.0985 - accuracy: 0.0000e+00\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 656us/step - loss: 19.0510 - accuracy: 0.0000e+00\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 664us/step - loss: 19.0040 - accuracy: 0.0000e+00\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 925us/step - loss: 18.9575 - accuracy: 0.0000e+00\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.9116 - accuracy: 0.0000e+00\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.8661 - accuracy: 0.0000e+00\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 804us/step - loss: 18.8212 - accuracy: 0.0000e+00\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 742us/step - loss: 18.7768 - accuracy: 0.0000e+00\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.7329 - accuracy: 0.0000e+00\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.6895 - accuracy: 0.0000e+00\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.6466 - accuracy: 0.0000e+00\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 763us/step - loss: 18.6042 - accuracy: 0.0000e+00\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 696us/step - loss: 18.5622 - accuracy: 0.0000e+00\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 660us/step - loss: 18.5207 - accuracy: 0.0000e+00\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 743us/step - loss: 18.4797 - accuracy: 0.0000e+00\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 674us/step - loss: 18.4391 - accuracy: 0.0000e+00\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 701us/step - loss: 18.3990 - accuracy: 0.0000e+00\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 692us/step - loss: 18.3594 - accuracy: 0.0000e+00\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 762us/step - loss: 18.3202 - accuracy: 0.0000e+00\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 18.2814 - accuracy: 0.0000e+00\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 806us/step - loss: 18.2431 - accuracy: 0.0000e+00\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 706us/step - loss: 18.2052 - accuracy: 0.0000e+00\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 847us/step - loss: 18.1678 - accuracy: 0.0000e+00\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 829us/step - loss: 18.1307 - accuracy: 0.0000e+00\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 815us/step - loss: 18.0941 - accuracy: 0.0000e+00\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 663us/step - loss: 18.0579 - accuracy: 0.0000e+00\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 746us/step - loss: 18.0221 - accuracy: 0.0000e+00\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 796us/step - loss: 17.9867 - accuracy: 0.0000e+00\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 940us/step - loss: 17.9517 - accuracy: 0.0000e+00\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 727us/step - loss: 17.9171 - accuracy: 0.0000e+00\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 810us/step - loss: 17.8828 - accuracy: 0.0000e+00\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 935us/step - loss: 17.8490 - accuracy: 0.0000e+00\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 819us/step - loss: 17.8156 - accuracy: 0.0000e+00\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 736us/step - loss: 17.7825 - accuracy: 0.0000e+00\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 776us/step - loss: 17.7498 - accuracy: 0.0000e+00\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 17.7174 - accuracy: 0.0000e+00\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 970us/step - loss: 17.6855 - accuracy: 0.0000e+00\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 802us/step - loss: 17.6539 - accuracy: 0.0000e+00\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 786us/step - loss: 17.6226 - accuracy: 0.0000e+00\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 760us/step - loss: 17.5917 - accuracy: 0.0000e+00\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 781us/step - loss: 17.5611 - accuracy: 0.0000e+00\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 796us/step - loss: 17.5309 - accuracy: 0.0000e+00\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 782us/step - loss: 17.5011 - accuracy: 0.0000e+00\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 768us/step - loss: 17.4715 - accuracy: 0.0000e+00\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 783us/step - loss: 17.4423 - accuracy: 0.0000e+00\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 825us/step - loss: 17.4135 - accuracy: 0.0000e+00\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 727us/step - loss: 17.3849 - accuracy: 0.0000e+00\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 920us/step - loss: 17.3567 - accuracy: 0.0000e+00\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 830us/step - loss: 17.3288 - accuracy: 0.0000e+00\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 758us/step - loss: 17.3012 - accuracy: 0.0000e+00\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 819us/step - loss: 17.2739 - accuracy: 0.0000e+00\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 820us/step - loss: 17.2469 - accuracy: 0.0000e+00\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 826us/step - loss: 17.2203 - accuracy: 0.0000e+00\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 749us/step - loss: 17.1939 - accuracy: 0.0000e+00\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 691us/step - loss: 17.1678 - accuracy: 0.0000e+00\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 817us/step - loss: 17.1420 - accuracy: 0.0000e+00\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 816us/step - loss: 17.1165 - accuracy: 0.0000e+00\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 659us/step - loss: 17.0913 - accuracy: 0.0000e+00\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 887us/step - loss: 17.0664 - accuracy: 0.0000e+00\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 819us/step - loss: 17.0418 - accuracy: 0.0000e+00\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 705us/step - loss: 17.0174 - accuracy: 0.0000e+00\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 674us/step - loss: 16.9933 - accuracy: 0.0000e+00\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 787us/step - loss: 16.9695 - accuracy: 0.0000e+00\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 894us/step - loss: 16.9460 - accuracy: 0.0000e+00\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 699us/step - loss: 16.9227 - accuracy: 0.0000e+00\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 767us/step - loss: 16.8997 - accuracy: 0.0000e+00\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 750us/step - loss: 16.8769 - accuracy: 0.0000e+00\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 817us/step - loss: 16.8544 - accuracy: 0.0000e+00\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 819us/step - loss: 16.8321 - accuracy: 0.0000e+00\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 985us/step - loss: 16.8102 - accuracy: 0.0000e+00\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 807us/step - loss: 16.7884 - accuracy: 0.0000e+00\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 871us/step - loss: 16.7669 - accuracy: 0.0000e+00\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 752us/step - loss: 16.7456 - accuracy: 0.0000e+00\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 827us/step - loss: 16.7246 - accuracy: 0.0000e+00\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 756us/step - loss: 16.7038 - accuracy: 0.0000e+00\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 847us/step - loss: 16.6833 - accuracy: 0.0000e+00\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 838us/step - loss: 16.6629 - accuracy: 0.0000e+00\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 897us/step - loss: 16.6428 - accuracy: 0.0000e+00\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 687us/step - loss: 16.6230 - accuracy: 0.0000e+00\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 812us/step - loss: 16.6033 - accuracy: 0.0000e+00\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 728us/step - loss: 16.5839 - accuracy: 0.0000e+00\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 695us/step - loss: 16.5647 - accuracy: 0.0000e+00\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 740us/step - loss: 16.5457 - accuracy: 0.0000e+00\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 685us/step - loss: 16.5269 - accuracy: 0.0000e+00\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 918us/step - loss: 16.5084 - accuracy: 0.0000e+00\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 640us/step - loss: 16.4900 - accuracy: 0.0000e+00\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 790us/step - loss: 16.4719 - accuracy: 0.0000e+00\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 697us/step - loss: 16.4539 - accuracy: 0.0000e+00\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 758us/step - loss: 16.4362 - accuracy: 0.0000e+00\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 829us/step - loss: 16.4187 - accuracy: 0.0000e+00\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 670us/step - loss: 16.4013 - accuracy: 0.0000e+00\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 677us/step - loss: 16.3842 - accuracy: 0.0000e+00\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 936us/step - loss: 16.3672 - accuracy: 0.0000e+00\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.3504 - accuracy: 0.0000e+00\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 648us/step - loss: 16.3339 - accuracy: 0.0000e+00\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 886us/step - loss: 16.3175 - accuracy: 0.0000e+00\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 710us/step - loss: 16.3013 - accuracy: 0.0000e+00\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 779us/step - loss: 16.2853 - accuracy: 0.0000e+00\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 826us/step - loss: 16.2694 - accuracy: 0.0000e+00\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 942us/step - loss: 16.2538 - accuracy: 0.0000e+00\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 808us/step - loss: 16.2383 - accuracy: 0.0000e+00\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 742us/step - loss: 16.2230 - accuracy: 0.0000e+00\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 752us/step - loss: 16.2078 - accuracy: 0.0000e+00\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 959us/step - loss: 16.1929 - accuracy: 0.0000e+00\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 901us/step - loss: 16.1781 - accuracy: 0.0000e+00\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 687us/step - loss: 16.1634 - accuracy: 0.0000e+00\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 921us/step - loss: 16.1489 - accuracy: 0.0000e+00\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 710us/step - loss: 16.1346 - accuracy: 0.0000e+00\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 790us/step - loss: 16.1205 - accuracy: 0.0000e+00\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 937us/step - loss: 16.1065 - accuracy: 0.0000e+00\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 984us/step - loss: 16.0927 - accuracy: 0.0000e+00\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 926us/step - loss: 16.0790 - accuracy: 0.0000e+00\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 830us/step - loss: 16.0655 - accuracy: 0.0000e+00\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 806us/step - loss: 16.0521 - accuracy: 0.0000e+00\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 821us/step - loss: 16.0389 - accuracy: 0.0000e+00\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 682us/step - loss: 16.0259 - accuracy: 0.0000e+00\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 894us/step - loss: 16.0129 - accuracy: 0.0000e+00\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 789us/step - loss: 16.0002 - accuracy: 0.0000e+00\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 943us/step - loss: 15.9875 - accuracy: 0.0000e+00\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 900us/step - loss: 15.9750 - accuracy: 0.0000e+00\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 785us/step - loss: 15.9627 - accuracy: 0.0000e+00\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 710us/step - loss: 15.9505 - accuracy: 0.0000e+00\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 952us/step - loss: 15.9384 - accuracy: 0.0000e+00\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 942us/step - loss: 15.9265 - accuracy: 0.0000e+00\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 818us/step - loss: 15.9147 - accuracy: 0.0000e+00\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 776us/step - loss: 15.9030 - accuracy: 0.0000e+00\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 914us/step - loss: 15.8915 - accuracy: 0.0000e+00\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 15.8801 - accuracy: 0.0000e+00\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 757us/step - loss: 15.8688 - accuracy: 0.0000e+00\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 845us/step - loss: 15.8576 - accuracy: 0.0000e+00\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 689us/step - loss: 15.8466 - accuracy: 0.0000e+00\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 655us/step - loss: 15.8357 - accuracy: 0.0000e+00\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 654us/step - loss: 15.8249 - accuracy: 0.0000e+00\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 816us/step - loss: 15.8143 - accuracy: 0.0000e+00\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 806us/step - loss: 15.8038 - accuracy: 0.0000e+00\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 812us/step - loss: 15.7933 - accuracy: 0.0000e+00\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 801us/step - loss: 15.7830 - accuracy: 0.0000e+00\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 933us/step - loss: 15.7729 - accuracy: 0.0000e+00\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 750us/step - loss: 15.7628 - accuracy: 0.0000e+00\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 706us/step - loss: 15.7528 - accuracy: 0.0000e+00\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 674us/step - loss: 15.7430 - accuracy: 0.0000e+00\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 939us/step - loss: 15.7332 - accuracy: 0.0000e+00\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 903us/step - loss: 15.7236 - accuracy: 0.0000e+00\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 916us/step - loss: 15.7141 - accuracy: 0.0000e+00\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 688us/step - loss: 15.7047 - accuracy: 0.0000e+00\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 769us/step - loss: 15.6954 - accuracy: 0.0000e+00\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 929us/step - loss: 15.6862 - accuracy: 0.0000e+00\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 939us/step - loss: 15.6771 - accuracy: 0.0000e+00\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 860us/step - loss: 15.6681 - accuracy: 0.0000e+00\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 698us/step - loss: 15.6592 - accuracy: 0.0000e+00\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 827us/step - loss: 15.6504 - accuracy: 0.0000e+00\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 823us/step - loss: 15.6417 - accuracy: 0.0000e+00\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 945us/step - loss: 15.6332 - accuracy: 0.0000e+00\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 858us/step - loss: 15.6247 - accuracy: 0.0000e+00\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 782us/step - loss: 15.6163 - accuracy: 0.0000e+00\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 782us/step - loss: 15.6080 - accuracy: 0.0000e+00\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 784us/step - loss: 15.5997 - accuracy: 0.0000e+00\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 805us/step - loss: 15.5916 - accuracy: 0.0000e+00\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 790us/step - loss: 15.5836 - accuracy: 0.0000e+00\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 841us/step - loss: 15.5757 - accuracy: 0.0000e+00\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 841us/step - loss: 15.5678 - accuracy: 0.0000e+00\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 968us/step - loss: 15.5600 - accuracy: 0.0000e+00\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 814us/step - loss: 15.5524 - accuracy: 0.0000e+00\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 814us/step - loss: 15.5448 - accuracy: 0.0000e+00\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 777us/step - loss: 15.5373 - accuracy: 0.0000e+00\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 677us/step - loss: 15.5299 - accuracy: 0.0000e+00\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 757us/step - loss: 15.5225 - accuracy: 0.0000e+00\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 745us/step - loss: 15.5153 - accuracy: 0.0000e+00\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 760us/step - loss: 15.5081 - accuracy: 0.0000e+00\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 867us/step - loss: 15.5010 - accuracy: 0.0000e+00\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 750us/step - loss: 15.4940 - accuracy: 0.0000e+00\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 833us/step - loss: 15.4871 - accuracy: 0.0000e+00\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 736us/step - loss: 15.4802 - accuracy: 0.0000e+00\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 807us/step - loss: 15.4735 - accuracy: 0.0000e+00\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 807us/step - loss: 15.4668 - accuracy: 0.0000e+00\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 813us/step - loss: 15.4601 - accuracy: 0.0000e+00\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 691us/step - loss: 15.4536 - accuracy: 0.0000e+00\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 831us/step - loss: 15.4471 - accuracy: 0.0000e+00\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 868us/step - loss: 15.4407 - accuracy: 0.0000e+00\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 735us/step - loss: 15.4344 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa89872a278>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_shape=(1,)))\n",
    "model.compile(loss='mean_squared_error', metrics=['accuracy'], optimizer='sgd')\n",
    "model.fit(\n",
    "    x, y, epochs=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.077126]]\n",
      "[[5.3408756]]\n"
     ]
    }
   ],
   "source": [
    "b = model.predict([0])\n",
    "print(b)\n",
    "m = (model.predict([2]) - b ) / 2\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have an idea? Excellent! Please shut down the kernel before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
